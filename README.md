# Attention Is All You Need

**NOTE:** this is work in progress,

This repository contains an implementation of the 'Attention Is All You Need' paper. The model is trained on the WMT German-English dataset. This implementation is mainly used for me to learn about transformers and attention

[Google Colab notebook here](https://colab.research.google.com/drive/1N3MUcCd-SUh0ne9P5c97Ib3gPXZ_sgcQ?usp=sharing).

## Model Architecture
**TODO:** Add details about the model architecture

## Performance
**TODO:** Add details about the performance

## Questions
- Can I limit vocab size somehow?
- What is the current sota score?
- What is an acceptable score that can be achieved in 10h?
- How to calculate the score, what metrics are useful?

## References
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Transformer explainer](https://poloclub.github.io/transformer-explainer/)
- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
- [WMT'16 multimodal translation task](http://www.statmt.org/wmt16/multimodal-task.html)
- [llm visualization](https://bbycroft.net/llm), this is not an llm, but the transformer is nicely shown
- [Transformer with PyTorch](https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch?dc_referrer=https%3A%2F%2Fwww.google.com%2F)
- [How Many Layers and Why? An Analysis of the Model Depth in Transformers](https://aclanthology.org/2021.acl-srw.23.pdf)
