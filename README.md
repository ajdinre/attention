# Attention Is All You Need

This repository features my implementation of the 'Attention Is All You Need' paper. The model is trained on the WMT German-English dataset and is aimed at neural machine translation. While this implementation is primarily for educational purposes, it serves as a hands-on way for me to learn the details of transformers and different attention mechanisms.

## Model Architecture
**TODO:** Add details about the model architecture.

## Performance
Please note that the current implementation and hyperparameters are not optimized. I plan to refine them and run this on actually good hardware.
**TODO:** Find ways to benchmark.

## References
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Transformer explainer](https://poloclub.github.io/transformer-explainer/)
- [WMT'16 multimodal translation task](http://www.statmt.org/wmt16/multimodal-task.html)
- [llm visualization](https://bbycroft.net/llm), this is not an llm, but the transformer is nicely shown