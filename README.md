# Attention Is All You Need

NOTE: this is work in progress

This repository features my implementation of the 'Attention Is All You Need' paper. The model is trained on the WMT German-English dataset and is aimed at neural machine translation. While this implementation is primarily for educational purposes, it serves as a hands-on way for me to learn the details of transformers and different attention mechanisms.

[Google Colab notebook here](https://colab.research.google.com/drive/1N3MUcCd-SUh0ne9P5c97Ib3gPXZ_sgcQ?usp=sharing).

## Model Architecture
**TODO:** Add details about the model architecture.

## Performance
Please note that the current implementation and hyperparameters are not optimized. I plan to refine them and run this on actually good hardware.
**TODO:** Find ways to benchmark.

## Questions
- Can I even limit vocab size?

## References
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Transformer explainer](https://poloclub.github.io/transformer-explainer/)
- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
- [WMT'16 multimodal translation task](http://www.statmt.org/wmt16/multimodal-task.html)
- [llm visualization](https://bbycroft.net/llm), this is not an llm, but the transformer is nicely shown
- [Transformer with PyTorch](https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch?dc_referrer=https%3A%2F%2Fwww.google.com%2F)
